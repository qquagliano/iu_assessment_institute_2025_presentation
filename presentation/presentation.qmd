---
# Bibliography opts
bibliography: "./refs/complete.bib"
nocite: "@*" # Cites all items present in bibliography
cite-method: citeproc
citeproc: true
csl: "../resources/bibs/apa.csl"
link-citations: true
link-bibliography: true
---
```{r}
#| lst-label: lst-r-libraries
#| lst-cap: "R libraries"

base::library(package = knitr) # For rendering document
base::library(package = quarto) # For rendering document

# Write all package citations to bib file for citation
knitr::write_bib(
  x    = base::.packages(),
  file = "./refs/r_packages.bib"
)
```

```{bash}
#| context: setup
#| lst-label: lst-citing
#| lst-cap: "Bibliography and citations"

# Check if a complete bib file already exists
if [ -f ./refs/complete.bib ]; then
  rm ./refs/complete.bib
fi

# Make an empty bib file
touch ./refs/complete.bib

# Append R package citation to bib file
if [ -f ./refs/r_packages.bib ]; then
  cat ./refs/r_packages.bib >>./refs/complete.bib
fi

# Add any additional citations to bib file
if [ -f ./refs/lit_review.bib ]; then
  cat ./refs/lit_review.bib >>./refs/complete.bib
fi
```

{{< pagebreak >}}

# Introduction {#sec-intro}

## Follow Along {#sec-follow-along}

![](./imgs/qr-code.png){width=50%}

## Motivation {#sec-motivation}

- *Valid* classroom assessment is **hard**

- Student perceptions on evaluations are **unclear**

- Student's have opinions on the *instructor*, less on *content*

- How can I bring students into the **process** of how I "grade" them

## Value of Student Buy-in {#sec-student-buy-in}

- Students may see assessment as accountability at best, irrelevant at worst
[@fletcher_faculty_2012; @brown_students_2008]

- Trust is important so they can learn from assessment [@chu_students_2014;
@carless_trust_2009; @leighton_pedagogical_2018]

- The right, little "moves" can help make that trust more tangible
[@felten_building_2023]

# My Implementation {#sec-my-implementation}

## Assessment at Regular Interval {#sec-assess-regular-interval}

- I already had consistent weekly quizzes as a mainstay in my courses

- Associated with good effects on retention [@thomas_should_2020;
@haigh_sustaining_2007]

- When corrective feedback is fast, supports better self-correction
[@marcell_effectiveness_2008]

- My existing structure was ripe for addition

## Embed in the Assessment {#sec-embed}

- 10-question multiple-choice quiz, open-note, relatively low stakes

- Embed in 1 - 2 quant/qual questions to assess "accuracy" of quiz

- Aim to capture an immediate "state"-like feeling, knee-jerk reaction (similar
to @spielberger_state-trait_2012)

## Format of the Question {#sec-id}

- Research on face validity from the student perspective is sparse at times

- Does show it's importance in maintaining relevance to the student
[@hollissawyer_potential_2008; @sato_test-taker_2015]

- Some examples:
  - "Rate how close this quiz was to what we learned in class last week (1 - 5)"
  - "Do you feel like this content was covered in class? (yes / no)"
  - "Explain whether you feel this was representative of what you know (open)"
  - "What other topic were you expecting to show up that didn't (open)"

## Debrief and Analysis {#sec-debrief-analysis}

- If easily accessible through LMS, may display bar graphs on board for
numerical questions

- Look for common vocabulary in the open responses

- Use the insights from the qualitative to follow-up on the quantitative

# Advice for Your Implementation {#sec-your-implementation}

## Things to Avoid {#sec-things-to-avoid}

- Creating culture of arguing for points

- Students aren't SMEs, you don't need to treat them like one!

- Only using quant or qual - use both!

- Trying to do too much!

{{< pagebreak >}}

## References {#sec-refs}

::: {#refs}
:::
